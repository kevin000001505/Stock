from auth import check_authentication, show_logout_button
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from stock import StockDataFetcher
import datetime
import time
import config
import json
import re
import pytz
from pathlib import Path


BASE_COLUMNS = ["ä»£è™Ÿ", "åç¨±", "ç”¢æ¥­åˆ¥"]
# Keep auto-fetch related configuration together
AUTO_FETCH_LOG_PATH = Path("auto_fetch_log.json")
AUTO_FETCH_HOUR = 18
AUTO_FETCH_MAX_ATTEMPTS = 3
AUTO_FETCH_TIMEZONE = pytz.timezone("Asia/Taipei")
USER_PREFS_PATH = Path("user_preferences.json")


def _read_user_preferences_store() -> dict:
    if USER_PREFS_PATH.exists():
        try:
            with USER_PREFS_PATH.open("r", encoding="utf-8") as file:
                data = json.load(file)
                if isinstance(data, dict):
                    return data
        except (json.JSONDecodeError, OSError):
            return {}
    return {}


def load_user_preferences(username: str) -> dict:
    if not username:
        return {}
    store = _read_user_preferences_store()
    prefs = store.get(username, {})
    return prefs.copy() if isinstance(prefs, dict) else {}


def save_user_preferences(username: str, preferences: dict) -> None:
    if not username:
        return
    store = _read_user_preferences_store()
    store[username] = preferences
    try:
        with USER_PREFS_PATH.open("w", encoding="utf-8") as file:
            json.dump(store, file, ensure_ascii=False, indent=2)
    except OSError:
        pass


check_authentication()

st.set_page_config(page_title="Stock Data Analysis", layout="wide")

current_username = st.session_state.get("username", "")
user_prefs = load_user_preferences(current_username)

stock_type_options = ["ä¸Šå¸‚", "ä¸Šæ«ƒ"]
stock_type_default = user_prefs.get("stock_type", stock_type_options[0])
if stock_type_default not in stock_type_options:
    stock_type_default = stock_type_options[0]

extrema_options = ["åº•è°·", "é ‚å³°", "å…©è€…"]
extrema_default = user_prefs.get("extrema_display", extrema_options[0])
if extrema_default not in extrema_options:
    extrema_default = extrema_options[0]

reverse_options = ["åšå¤š", "åšç©º"]
reverse_default = user_prefs.get("reverse_display", reverse_options[0])
if reverse_default not in reverse_options:
    reverse_default = reverse_options[0]


def _clamp_int(value, minimum, maximum, fallback):
    try:
        int_value = int(value)
    except (TypeError, ValueError):
        return fallback
    return max(minimum, min(maximum, int_value))


num_extrema_default = _clamp_int(user_prefs.get("num_extrema", 2), 2, 10, 2)
window_size_default = _clamp_int(user_prefs.get("window_size", 1), 1, 10, 1)
display_days_default = _clamp_int(user_prefs.get("display_days", 20), 5, 100, 20)

selected_categories_map = user_prefs.get("selected_categories", {})
if not isinstance(selected_categories_map, dict):
    selected_categories_map = {}

observed_default_codes = user_prefs.get("observed_codes", [])
if not isinstance(observed_default_codes, list):
    observed_default_codes = []
observed_default_text = "\n".join(observed_default_codes)

# Initialize the StockDataFetcher
stock_fetcher = StockDataFetcher()

st.sidebar.header("åƒæ•¸èª¿æ•´")

# Stock type selection
stock_type = st.sidebar.radio(
    "å¸‚å ´",
    stock_type_options,
    index=stock_type_options.index(stock_type_default),
)

# Number of extrema to find
num_extrema = st.sidebar.slider("é¡¯ç¤ºåº•æˆ–é ‚æ•¸é‡", 2, 10, num_extrema_default)

# Extrema type - Modified to include "Both" option
extrema_display = st.sidebar.radio(
    "ç¯©é¸æ–¹å¼",
    extrema_options,
    index=extrema_options.index(extrema_default),
)

# Map Chinese to English for internal use
extrema_mapping = {"åº•è°·": "Minima", "é ‚å³°": "Maxima", "å…©è€…": "Both"}

extrema_type = extrema_mapping[extrema_display]

# Reverse option - Changed to radio button with Chinese labels
reverse_display = st.sidebar.radio(
    "æ¯”è¼ƒæ–¹å‘",
    reverse_options,
    index=reverse_options.index(reverse_default),
)

# Map Chinese to English/Boolean for internal use
reverse_option = True if reverse_display == "åšç©º" else False

# Moving Average controls in sidebar
st.sidebar.header("ç§»å‹•å¹³å‡æ•¸")
window_size = st.sidebar.slider("å¤©æ•¸", 1, 10, window_size_default)

# Add data display range control
st.sidebar.header("è³‡æ–™é¡¯ç¤ºç¯„åœ")
display_days = st.sidebar.slider(
    "é¡¯ç¤ºå¤©æ•¸",
    5,
    100,
    display_days_default,
    help="é¸æ“‡è¦é¡¯ç¤ºçš„è³‡æ–™å¤©æ•¸",
)


def get_today_date():
    """Get today's date (Taipei time) in MM/DD format."""
    today = datetime.datetime.now(AUTO_FETCH_TIMEZONE)
    return f"{today.month:02d}/{today.day:02d}"


def load_auto_fetch_log() -> dict:
    if AUTO_FETCH_LOG_PATH.exists():
        try:
            with AUTO_FETCH_LOG_PATH.open("r", encoding="utf-8") as file:
                data = json.load(file)
        except (json.JSONDecodeError, OSError):
            data = {}
    else:
        data = {}

    data.setdefault("listed", {})
    data.setdefault("counter", {})
    return data


def save_auto_fetch_log(log: dict) -> None:
    try:
        with AUTO_FETCH_LOG_PATH.open("w", encoding="utf-8") as file:
            json.dump(log, file, ensure_ascii=False, indent=2)
    except OSError:
        pass


def maybe_auto_fetch(
    market_key: str,
    market_df: pd.DataFrame,
    current_date_label: str,
) -> tuple[pd.DataFrame, list[str]]:
    """Attempt to auto-fetch data after 18:00 with retry tracking."""

    now = datetime.datetime.now(AUTO_FETCH_TIMEZONE)
    messages: list[str] = []

    if now.hour < AUTO_FETCH_HOUR:
        return market_df, messages

    today_key = now.date().isoformat()
    log = load_auto_fetch_log()
    market_log = log.setdefault(market_key, {})
    entry = market_log.get(today_key, {"attempts": 0})

    if current_date_label in market_df.columns:
        if today_key in market_log:
            del market_log[today_key]
            save_auto_fetch_log(log)
        return market_df, messages

    if entry.get("status") == "skipped":
        messages.append("ä»Šæ—¥è‡ªå‹•æ›´æ–°å·²è·³éã€‚")
        return market_df, messages

    attempts = entry.get("attempts", 0)

    if attempts >= AUTO_FETCH_MAX_ATTEMPTS:
        entry["status"] = "skipped"
        market_log[today_key] = entry
        save_auto_fetch_log(log)
        messages.append("ä»Šæ—¥è‡ªå‹•æ›´æ–°å·²é”å˜—è©¦ä¸Šé™ï¼Œè·³éã€‚")
        return market_df, messages

    last_attempt = entry.get("last_attempt")
    if last_attempt:
        try:
            last_attempt_dt = datetime.datetime.fromisoformat(last_attempt)
        except ValueError:
            last_attempt_dt = None
        if last_attempt_dt and now - last_attempt_dt < datetime.timedelta(hours=1):
            wait_minutes = 60 - int((now - last_attempt_dt).total_seconds() // 60)
            messages.append(f"ç­‰å¾…é‡æ–°å˜—è©¦è‡ªå‹•æ›´æ–°ï¼ˆç´„ {wait_minutes} åˆ†é˜å¾Œï¼‰ã€‚")
            return market_df, messages

    try:
        updated_df = stock_fetcher.fetch_stock_data(market_df, market_key)
    except Exception as exc:  # noqa: BLE001
        attempts += 1
        entry.update(
            {
                "attempts": attempts,
                "last_attempt": now.isoformat(),
                "status": "error",
                "error": str(exc),
            }
        )
        market_log[today_key] = entry
        save_auto_fetch_log(log)
        messages.append(f"è‡ªå‹•æ›´æ–°å¤±æ•—ï¼š{exc}")

        if attempts >= AUTO_FETCH_MAX_ATTEMPTS:
            entry["status"] = "skipped"
            messages.append("å·²é”è‡ªå‹•æ›´æ–°ä¸Šé™ï¼Œä»Šæ—¥ä¸å†å˜—è©¦ã€‚")
        return market_df, messages

    if current_date_label in updated_df.columns:
        if today_key in market_log:
            del market_log[today_key]
        save_auto_fetch_log(log)
        messages.append("è‡ªå‹•æ›´æ–°æˆåŠŸï¼Œå·²å–å¾—ä»Šæ—¥è³‡æ–™ã€‚")
        return updated_df, messages

    attempts += 1
    entry.update(
        {
            "attempts": attempts,
            "last_attempt": now.isoformat(),
            "status": "pending",
        }
    )
    market_log[today_key] = entry
    save_auto_fetch_log(log)
    messages.append("è‡ªå‹•æ›´æ–°å˜—è©¦å¾Œä»ç„¡ä»Šæ—¥è³‡æ–™ã€‚")

    if attempts >= AUTO_FETCH_MAX_ATTEMPTS:
        entry["status"] = "skipped"
        messages.append("å·²é”è‡ªå‹•æ›´æ–°ä¸Šé™ï¼Œä»Šæ—¥ä¸å†å˜—è©¦ã€‚")

    return market_df, messages


# Get current date dynamically
current_date = get_today_date()

# Check if current date data already exists
if stock_type == "ä¸Šå¸‚":
    market_df = stock_fetcher.listed_data
    market_key = "listed"
else:
    market_df = stock_fetcher.counter_data
    market_key = "counter"

market_df, auto_fetch_messages = maybe_auto_fetch(
    market_key=market_key,
    market_df=market_df,
    current_date_label=current_date,
)

has_current_data = current_date in market_df.columns
market_price_columns = [col for col in market_df.columns if col not in BASE_COLUMNS]
market_code_series = (
    market_df["ä»£è™Ÿ"].astype(str)
    if "ä»£è™Ÿ" in market_df.columns
    else pd.Series(dtype=str)
)

# Button to fetch latest data
if st.sidebar.button("ç²å–ä»Šæ—¥è‚¡ç¥¨è³‡æ–™"):
    if has_current_data:
        st.info(f"Data for {current_date} already exists. No fetch needed.")
    else:
        # Create placeholders for progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        time_text = st.empty()
        detail_text = st.empty()

        def update_progress(
            message: str, current: int, total: int, estimated_remaining: float
        ):
            """Callback function to update progress UI"""
            progress = current / total if total > 0 else 0
            progress_bar.progress(progress)
            status_text.text(f"{message} ({current}/{total})")

            if estimated_remaining > 0:
                minutes = int(estimated_remaining // 60)
                seconds = int(estimated_remaining % 60)
                if minutes > 0:
                    time_text.text(f"â±ï¸ é ä¼°å‰©é¤˜æ™‚é–“: {minutes} åˆ† {seconds} ç§’")
                else:
                    time_text.text(f"â±ï¸ é ä¼°å‰©é¤˜æ™‚é–“: {seconds} ç§’")

            # Show processing rate
            if current > 0:
                detail_text.text(f"ğŸ“Š è™•ç†é€²åº¦: {progress*100:.1f}% | æ¯ç­†ç´„ 10 ç§’")

        try:
            market_df = stock_fetcher.fetch_stock_data(
                market_df, market_key, progress_callback=update_progress
            )

            progress_bar.progress(1.0)
            status_text.empty()
            time_text.empty()
            detail_text.empty()
            st.success("è³‡æ–™æ›´æ–°æˆåŠŸï¼")
            time.sleep(1)
            st.rerun()

        except Exception as e:
            progress_bar.empty()
            status_text.empty()
            time_text.empty()
            detail_text.empty()
            st.error(f"ç²å–è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

# Display current data status
if has_current_data:
    st.sidebar.success(f"âœ… Data for {current_date} is available")
else:
    st.sidebar.warning(f"âš ï¸ Data for {current_date} is missing")

for message in auto_fetch_messages:
    if any(keyword in message for keyword in ["å¤±æ•—", "ä¸Šé™", "è·³é"]):
        st.sidebar.warning(message)
    elif "æˆåŠŸ" in message:
        st.sidebar.success(message)
    else:
        st.sidebar.info(message)


# Show data overview with filtered dataframe based on extrema results
st.header(f"{stock_type} è‚¡ç¥¨æ•¸æ“šæ¦‚è¦½")


def prepare_series(
    values: np.ndarray,
    labels: np.ndarray,
    window_size: int,
    display_days: int,
) -> tuple[np.ndarray, np.ndarray]:
    """Apply moving average and trim to the requested display window."""

    processed_values = values
    processed_labels = labels

    if window_size > 1 and values.size >= window_size:
        kernel = np.ones(window_size) / window_size
        processed_values = np.convolve(values, kernel, mode="valid")
        processed_labels = labels[window_size - 1 :]

    if display_days and processed_values.size > display_days:
        processed_values = processed_values[-display_days:]
        processed_labels = processed_labels[-display_days:]

    return processed_values, processed_labels


def analyze_stock_condition(
    values: np.ndarray,
    labels: np.ndarray,
    window_size: int,
    display_days: int,
    extrema_type: str,
    num_extrema: int,
    reverse_option: bool,
) -> tuple[bool, np.ndarray, np.ndarray, dict[str, list[int]]]:
    processed_values, processed_labels = prepare_series(
        values, labels, window_size, display_days
    )

    if processed_values.size < 3:
        return False, processed_values, processed_labels, {}

    series_for_analysis = processed_values.tolist()
    extrema_details: dict[str, list[int]] = {}

    try:
        if extrema_type == "Both":
            minima_result, minima_locations = stock_fetcher.find_local_extrema(
                series_for_analysis,
                find_type="minima",
                number_of_extrema=num_extrema,
                reverse=reverse_option,
            )
            maxima_result, maxima_locations = stock_fetcher.find_local_extrema(
                series_for_analysis,
                find_type="maxima",
                number_of_extrema=num_extrema,
                reverse=reverse_option,
            )
            extrema_details["minima"] = minima_locations
            extrema_details["maxima"] = maxima_locations
            matches = (
                bool(minima_result)
                and bool(maxima_result)
                and bool(minima_locations)
                and bool(maxima_locations)
            )
        else:
            single_type = extrema_type.lower()
            result, locations = stock_fetcher.find_local_extrema(
                series_for_analysis,
                find_type=single_type,
                number_of_extrema=num_extrema,
                reverse=reverse_option,
            )
            extrema_details[single_type] = locations
            matches = bool(result) and bool(locations)
    except Exception:  # noqa: BLE001
        return False, processed_values, processed_labels, {}

    return matches, processed_values, processed_labels, extrema_details


category_options = (
    sorted(market_df["ç”¢æ¥­åˆ¥"].dropna().unique())
    if "ç”¢æ¥­åˆ¥" in market_df.columns
    else []
)

if stock_type in selected_categories_map:
    stored_categories_for_market = selected_categories_map[stock_type]
    if not isinstance(stored_categories_for_market, list):
        stored_categories_for_market = []
    preselected_categories = [
        category
        for category in stored_categories_for_market
        if category in category_options
    ]
    if not preselected_categories and stored_categories_for_market and category_options:
        preselected_categories = category_options
else:
    preselected_categories = category_options

selected_categories = st.sidebar.multiselect(
    "ç”¢æ¥­åˆ¥",
    options=category_options,
    default=preselected_categories,
    placeholder="é¸æ“‡æ¬²åˆ†æçš„ç”¢æ¥­åˆ¥",
    help="ä¾ç…§è‚¡ç¥¨ç”¢æ¥­åˆ¥ç¯©é¸è³‡æ–™ï¼Œé è¨­ç‚ºå…¨éƒ¨ç”¢æ¥­",
)

if selected_categories:
    filtered_market_df = market_df[market_df["ç”¢æ¥­åˆ¥"].isin(selected_categories)].copy()
else:
    filtered_market_df = market_df.iloc[0:0].copy()

filtered_market_df.reset_index(drop=True, inplace=True)

st.sidebar.header("è§€å¯Ÿè‚¡ç¥¨")
observed_input = st.sidebar.text_area(
    "è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿæ¸…å–®",
    placeholder="ä¾‹å¦‚: 2330\n2603, 0050",
    help="ä½¿ç”¨é€—è™Ÿã€ç©ºç™½æˆ–æ›è¡Œåˆ†éš”å¤šå€‹ä»£è™Ÿï¼Œå¯åŒæ™‚è¿½è¹¤å¤šæª”è‚¡ç¥¨",
    value=observed_default_text,
)

observed_codes: list[str] = []
if observed_input:
    tokens = re.split(r"[\s,;]+", observed_input)
    observed_codes = [token.strip() for token in tokens if token.strip()]
    observed_codes = list(dict.fromkeys(observed_codes))

if observed_codes:
    results: list[dict[str, str]] = []
    satisfied_count = 0

    for code in observed_codes:
        result = {"ä»£è™Ÿ": code, "åç¨±": "-", "æœ€æ–°åƒ¹æ ¼": "-", "ç‹€æ…‹": "âš ï¸ æ‰¾ä¸åˆ°è‚¡ç¥¨"}

        if market_code_series.empty:
            results.append({**result, "ç‹€æ…‹": "âš ï¸ å°šæœªè¼‰å…¥å¸‚å ´è³‡æ–™"})
            continue

        match_mask = market_code_series == code

        if not match_mask.any():
            results.append(result)
            continue

        observed_row = market_df.loc[match_mask].iloc[0]
        stock_name_display = str(observed_row.get("åç¨±", ""))
        result["åç¨±"] = stock_name_display if stock_name_display else "-"

        if not market_price_columns:
            result["ç‹€æ…‹"] = "âš ï¸ ç„¡æ­·å²åƒ¹æ ¼è³‡æ–™"
            results.append(result)
            continue

        observed_prices = pd.to_numeric(
            observed_row[market_price_columns], errors="coerce"
        ).to_numpy(dtype=float)
        observed_labels = np.array(market_price_columns)
        valid_mask = ~np.isnan(observed_prices)

        if not valid_mask.any():
            result["ç‹€æ…‹"] = "âš ï¸ ç„¡æœ‰æ•ˆåƒ¹æ ¼"
            results.append(result)
            continue

        latest_price = observed_prices[valid_mask][-1]
        result["æœ€æ–°åƒ¹æ ¼"] = (
            f"{latest_price:.2f}" if not np.isnan(latest_price) else "-"
        )

        if valid_mask.sum() < 3:
            result["ç‹€æ…‹"] = "âŒ› è³‡æ–™ä¸è¶³"
            results.append(result)
            continue

        valid_values = observed_prices[valid_mask]
        valid_labels = observed_labels[valid_mask]
        matches, processed_values, _, _ = analyze_stock_condition(
            valid_values,
            valid_labels,
            window_size,
            display_days,
            extrema_type,
            num_extrema,
            reverse_option,
        )

        if processed_values.size < 3:
            result["ç‹€æ…‹"] = "âŒ› è³‡æ–™ä¸è¶³"
        elif matches:
            result["ç‹€æ…‹"] = "âœ… ç¬¦åˆ"
            satisfied_count += 1
        else:
            result["ç‹€æ…‹"] = "âŒ› å°šæœªç¬¦åˆ"

        results.append(result)

    if results:
        st.sidebar.caption(f"ç¬¦åˆæ¢ä»¶ï¼š{satisfied_count} / {len(results)}")
        results_df = pd.DataFrame(results)
        display_height = min(400, max(200, 48 * len(results)))
        st.sidebar.dataframe(
            results_df,
            use_container_width=True,
            height=display_height,
        )
else:
    results_df = pd.DataFrame(columns=["ä»£è™Ÿ", "åç¨±", "æœ€æ–°åƒ¹æ ¼", "ç‹€æ…‹"])

with st.sidebar.expander("åå¥½è¨­å®š"):
    store_clicked = st.button("ğŸ’¾ å„²å­˜ç›®å‰æ¢ä»¶", use_container_width=True)

if store_clicked and current_username:
    updated_categories_map = dict(selected_categories_map)
    updated_categories_map[stock_type] = selected_categories

    new_preferences = dict(user_prefs)
    new_preferences.update(
        {
            "stock_type": stock_type,
            "num_extrema": num_extrema,
            "extrema_display": extrema_display,
            "reverse_display": reverse_display,
            "window_size": window_size,
            "display_days": display_days,
            "observed_codes": list(observed_codes),
            "selected_categories": updated_categories_map,
        }
    )

    save_user_preferences(current_username, new_preferences)
    st.sidebar.success("âœ… å·²å„²å­˜åå¥½è¨­å®š")
    user_prefs = new_preferences
    selected_categories_map = updated_categories_map

display_df = pd.DataFrame(columns=BASE_COLUMNS)
price_columns = [col for col in filtered_market_df.columns if col not in BASE_COLUMNS]
total_considered = len(filtered_market_df)
has_category_selection = bool(selected_categories)

if total_considered and price_columns:
    numeric_prices = filtered_market_df.loc[:, price_columns].apply(
        pd.to_numeric, errors="coerce"
    )
    price_matrix = numeric_prices.to_numpy(dtype=float)
    price_labels = np.array(price_columns)

    filtered_records: list[dict] = []

    with st.spinner("Analyzing all stocks for positive trends..."):
        for row_idx in range(total_considered):
            row_prices = price_matrix[row_idx]
            valid_mask = ~np.isnan(row_prices)

            if valid_mask.sum() < 3:
                continue

            valid_values = row_prices[valid_mask]
            valid_labels = price_labels[valid_mask]

            matches_filter, processed_values, processed_labels, _ = (
                analyze_stock_condition(
                    valid_values,
                    valid_labels,
                    window_size,
                    display_days,
                    extrema_type,
                    num_extrema,
                    reverse_option,
                )
            )

            if not matches_filter:
                continue

            stock_info = filtered_market_df.loc[row_idx, BASE_COLUMNS].to_dict()
            record = {**stock_info}
            record.update(
                {
                    label: round(value, 2)
                    for label, value in zip(processed_labels.tolist(), processed_values)
                }
            )
            filtered_records.append(record)

    if filtered_records:
        display_df = pd.DataFrame(filtered_records)

        ordered_date_columns = [
            col
            for col in price_columns
            if col in display_df.columns and col not in BASE_COLUMNS
        ]

        if display_days and len(ordered_date_columns) > display_days:
            ordered_date_columns = ordered_date_columns[-display_days:]

        final_columns = BASE_COLUMNS + ordered_date_columns
        display_df = display_df.reindex(columns=final_columns)
        display_df.sort_values(by="ä»£è™Ÿ", inplace=True)
        display_df.reset_index(drop=True, inplace=True)
        display_df["ä»£è™Ÿ"] = display_df["ä»£è™Ÿ"].astype(str)
    else:
        display_df = pd.DataFrame(columns=BASE_COLUMNS)
else:
    if not has_category_selection and category_options:
        st.sidebar.warning("âš ï¸ è«‹é¸æ“‡è‡³å°‘ä¸€å€‹ç”¢æ¥­åˆ¥é€²è¡Œç¯©é¸")

if not display_df.empty:
    st.success(
        f"Found {len(display_df)} stocks with positive trends out of {total_considered} total stocks"
    )
else:
    if not has_category_selection and not category_options:
        st.info("ğŸš« æ­¤å¸‚å ´å°šæœªæä¾›ç”¢æ¥­åˆ¥è³‡æ–™")
    elif not has_category_selection:
        st.info("ğŸ‘ˆ åœ¨å´é‚Šæ¬„é¸æ“‡æ¬²åˆ†æçš„ç”¢æ¥­åˆ¥å¾Œå³å¯é¡¯ç¤ºçµæœ")
    elif total_considered == 0:
        st.error("âŒ é¸å®šçš„ç”¢æ¥­åˆ¥ç›®å‰æ²’æœ‰å¯ç”¨è‚¡ç¥¨è³‡æ–™")
    else:
        st.error("âŒ æ²’æœ‰è‚¡ç¥¨ç¬¦åˆæ‚¨è¨­å®šçš„æ¢ä»¶")

        # Show what the user was looking for
        st.info("**æ‚¨çš„ç¯©é¸æ¢ä»¶:**")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"- å¸‚å ´: **{stock_type}**")
            st.write(f"- ç¯©é¸æ–¹å¼: **{extrema_display}**")
            st.write(f"- æ¯”è¼ƒæ–¹å‘: **{reverse_display}**")
            st.write(
                "- ç”¢æ¥­åˆ¥: "
                + (
                    "å…¨éƒ¨"
                    if len(selected_categories) == len(category_options)
                    else ", ".join(selected_categories)
                )
            )
        with col2:
            st.write(f"- åº•/é ‚æ•¸é‡: **{num_extrema}**")
            st.write(f"- ç§»å‹•å¹³å‡: **{window_size} å¤©**")
            st.write(f"- é¡¯ç¤ºç¯„åœ: **{display_days} å¤©**")

        display_df = pd.DataFrame(columns=BASE_COLUMNS)

st.write(f"è‚¡ç¥¨æ•¸: {len(display_df)}")

# Display the filtered dataframe with scrolling capability
if len(display_df) > 0:
    # Add download button for filtered data
    if not display_df.empty:
        price_cols_for_download = [
            col for col in display_df.columns if col not in BASE_COLUMNS
        ]

        if price_cols_for_download:
            download_columns = ["ä»£è™Ÿ", "åç¨±", price_cols_for_download[-1]]
        else:
            download_columns = ["ä»£è™Ÿ", "åç¨±", "ç”¢æ¥­åˆ¥"]

        download_df = display_df.loc[:, download_columns].copy()

        download_df["ä»£è™Ÿ"] = download_df["ä»£è™Ÿ"].astype(str)
        # Convert to CSV
        csv = download_df.to_csv(index=False, encoding="utf-8-sig")

        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ç•¶æ—¥ç¬¦åˆè‚¡ç¥¨ (CSV)",
            data=csv.encode("utf-8-sig"),  # Add this encoding
            file_name=f"{stock_type}_{current_date}.csv",
            mime="text/csv; charset=utf-8",  # Specify charset
            help="ä¸‹è¼‰åŒ…å«è‚¡ç¥¨ä»£è™Ÿã€åç¨±å’Œæœ€æ–°åƒ¹æ ¼çš„è³‡æ–™",
        )

    # Make the dataframe clickable
    event = st.dataframe(
        display_df,
        use_container_width=True,
        height=400,
        on_select="rerun",
        selection_mode="single-row",
    )

    # Get the selected stock from dataframe click
    selected_stock_from_df = None
    if event.selection and len(event.selection.rows) > 0:
        selected_row_index = event.selection.rows[0]
        selected_stock_from_df = display_df.iloc[selected_row_index]["ä»£è™Ÿ"]
else:
    if has_category_selection and total_considered > 0:
        st.info("ğŸš« æ²’æœ‰è‚¡ç¥¨ç¬¦åˆæ‚¨çš„ç¯©é¸æ¢ä»¶")
    elif not has_category_selection:
        st.info("ğŸ‘ˆ åœ¨å´é‚Šæ¬„é¸æ“‡ç”¢æ¥­åˆ¥å¾Œå³å¯é¡¯ç¤ºè‚¡ç¥¨æ¸…å–®")
    else:
        st.info("ğŸš« æ­¤å¸‚å ´æš«æ™‚æ²’æœ‰å¯é¡¯ç¤ºçš„è‚¡ç¥¨è³‡æ–™")

# Stock filter (now works on the already filtered data)
st.header("è‚¡ç¥¨åˆ†æ")
stock_filter = st.text_input("Filter stocks by code or name", "")

filtered_df = display_df
if stock_filter and len(display_df) > 0:
    filtered_df = display_df[
        display_df["ä»£è™Ÿ"].str.contains(stock_filter, na=False)
        | display_df["åç¨±"].str.contains(stock_filter, na=False)
    ]

if not filtered_df.empty:
    # Use selected stock from dataframe click or selectbox
    if (
        selected_stock_from_df
        and selected_stock_from_df in filtered_df["ä»£è™Ÿ"].tolist()
    ):
        selected_stock = selected_stock_from_df
        # Update selectbox to show the clicked stock
        stock_options = filtered_df["ä»£è™Ÿ"].tolist()
        default_index = (
            stock_options.index(selected_stock)
            if selected_stock in stock_options
            else 0
        )
        selected_stock = st.selectbox(
            "Select a Stock", stock_options, index=default_index
        )
    else:
        # Allow user to select a stock normally
        selected_stock = st.selectbox("Select a Stock", filtered_df["ä»£è™Ÿ"].tolist())

    # Get the selected stock data - USE THE SAME RAW DATA SOURCE
    stock_row = market_df.loc[market_code_series == selected_stock].iloc[0]
    stock_name = stock_row["åç¨±"]

    st.subheader(f"{selected_stock} - {stock_name}")

    # Get the price data columns from original df
    price_columns = market_price_columns

    # Extract RAW price data for the selected stock
    raw_price_series = pd.to_numeric(stock_row[price_columns], errors="coerce")
    price_array = raw_price_series.to_numpy(dtype=float)
    price_labels = np.array(price_columns)

    valid_mask = ~np.isnan(price_array)
    valid_values = price_array[valid_mask]
    valid_labels = price_labels[valid_mask]

    # Find local extrema
    if valid_values.size >= 3:
        use_moving_average = window_size > 1 and valid_values.size >= window_size
        condition_met, processed_values, processed_labels, extrema_info = (
            analyze_stock_condition(
                valid_values,
                valid_labels,
                window_size,
                display_days,
                extrema_type,
                num_extrema,
                reverse_option,
            )
        )

        if processed_values.size < 3:
            st.warning("Not enough processed price data for analysis after trimming.")
        else:
            data_to_analyze = processed_values.tolist()
            labels_to_use = processed_labels.tolist()
            line_label = (
                f"Moving Average ({window_size})" if use_moving_average else "Price"
            )
            line_color = "orange" if use_moving_average else "steelblue"

            if extrema_type == "Both":
                minima_locations = extrema_info.get("minima", [])
                maxima_locations = extrema_info.get("maxima", [])

                if not condition_met:
                    st.warning("æ­¤è‚¡ç¥¨ç›®å‰å°šæœªç¬¦åˆæ¢ä»¶")

                if not minima_locations or not maxima_locations:
                    st.warning(f"âŒ ç„¡æ³•æ‰¾åˆ°è¶³å¤ çš„{extrema_display}é€²è¡Œåˆ†æ")
                    st.info(
                        f"æ­¤è‚¡ç¥¨åœ¨è¨­å®šæ¢ä»¶ä¸‹æ‰¾ä¸åˆ° {num_extrema} å€‹{extrema_display}"
                    )
                else:
                    plt.style.use("seaborn-v0_8")
                    fig, ax = plt.subplots(figsize=(14, 8))

                    x_positions = range(len(data_to_analyze))
                    x_labels = labels_to_use[: len(data_to_analyze)]

                    sns.lineplot(
                        x=x_positions,
                        y=data_to_analyze,
                        marker="o",
                        linewidth=3,
                        markersize=8,
                        color=line_color,
                        alpha=0.8,
                        ax=ax,
                        label=line_label,
                    )

                    minima_values = [data_to_analyze[loc] for loc in minima_locations]
                    ax.scatter(
                        minima_locations,
                        minima_values,
                        color="red",
                        s=200,
                        marker="v",
                        label="Local Minima",
                        zorder=5,
                        edgecolor="darkred",
                        linewidth=2,
                    )

                    if len(minima_locations) > 1:
                        ax.plot(
                            minima_locations,
                            minima_values,
                            color="red",
                            linestyle="--",
                            linewidth=2,
                            alpha=0.7,
                            label="Minima Trend Line",
                        )

                    for loc in minima_locations:
                        ax.annotate(
                            f"{data_to_analyze[loc]:.2f}",
                            xy=(loc, data_to_analyze[loc]),
                            xytext=(0, -20),
                            textcoords="offset points",
                            bbox=dict(boxstyle="round,pad=0.3", fc="red", alpha=0.8),
                            arrowprops=dict(
                                arrowstyle="->", connectionstyle="arc3,rad=0"
                            ),
                            fontsize=12,
                            color="white",
                            weight="bold",
                            ha="center",
                        )

                    maxima_values = [data_to_analyze[loc] for loc in maxima_locations]
                    ax.scatter(
                        maxima_locations,
                        maxima_values,
                        color="green",
                        s=200,
                        marker="^",
                        label="Local Maxima",
                        zorder=5,
                        edgecolor="darkgreen",
                        linewidth=2,
                    )

                    if len(maxima_locations) > 1:
                        ax.plot(
                            maxima_locations,
                            maxima_values,
                            color="green",
                            linestyle="--",
                            linewidth=2,
                            alpha=0.7,
                            label="Maxima Trend Line",
                        )

                    for loc in maxima_locations:
                        ax.annotate(
                            f"{data_to_analyze[loc]:.2f}",
                            xy=(loc, data_to_analyze[loc]),
                            xytext=(0, 20),
                            textcoords="offset points",
                            bbox=dict(boxstyle="round,pad=0.3", fc="green", alpha=0.8),
                            arrowprops=dict(
                                arrowstyle="->", connectionstyle="arc3,rad=0"
                            ),
                            fontsize=12,
                            color="white",
                            weight="bold",
                            ha="center",
                        )

                    ax.set_xlabel("Date", fontsize=14, fontweight="bold")
                    ax.set_ylabel("Price", fontsize=14, fontweight="bold")
                    ax.set_xticks(x_positions)
                    ax.set_xticklabels(x_labels, rotation=45, ha="right")
                    ax.grid(True, alpha=0.3, linestyle="--")
                    ax.legend(fontsize=12, loc="upper left")
                    ax.set_facecolor("#f8f9fa")
                    plt.tight_layout()

                    st.pyplot(fig)

                    st.subheader("è©³ç´°è³‡è¨Š")
                    col1, col2 = st.columns(2)

                    minima_data = [
                        {
                            "Date": labels_to_use[loc],
                            "Position": loc,
                            "Value": data_to_analyze[loc],
                            "Type": "Minimum",
                        }
                        for loc in minima_locations
                    ]
                    maxima_data = [
                        {
                            "Date": labels_to_use[loc],
                            "Position": loc,
                            "Value": data_to_analyze[loc],
                            "Type": "Maximum",
                        }
                        for loc in maxima_locations
                    ]

                    with col1:
                        if minima_data:
                            st.dataframe(
                                pd.DataFrame(minima_data), use_container_width=True
                            )
                        else:
                            st.info("No minima found")

                    with col2:
                        if maxima_data:
                            st.dataframe(
                                pd.DataFrame(maxima_data), use_container_width=True
                            )
                        else:
                            st.info("No maxima found")

            else:
                single_key = extrema_type.lower()
                locations = extrema_info.get(single_key, [])

                if not condition_met:
                    st.warning("æ­¤è‚¡ç¥¨ç›®å‰å°šæœªç¬¦åˆæ¢ä»¶")

                if not locations:
                    st.warning(f"âŒ ç„¡æ³•æ‰¾åˆ°è¶³å¤ çš„{extrema_display}é€²è¡Œåˆ†æ")
                    st.info(
                        f"æ­¤è‚¡ç¥¨åœ¨è¨­å®šæ¢ä»¶ä¸‹æ‰¾ä¸åˆ° {num_extrema} å€‹{extrema_display}"
                    )
                else:
                    plt.style.use("seaborn-v0_8")
                    fig, ax = plt.subplots(figsize=(14, 8))

                    x_positions = range(len(data_to_analyze))
                    x_labels = labels_to_use[: len(data_to_analyze)]

                    sns.lineplot(
                        x=x_positions,
                        y=data_to_analyze,
                        marker="o",
                        linewidth=3,
                        markersize=8,
                        color=line_color,
                        alpha=0.8,
                        ax=ax,
                        label=line_label,
                    )

                    extrema_values = [data_to_analyze[loc] for loc in locations]
                    if extrema_type == "Minima":
                        ax.scatter(
                            locations,
                            extrema_values,
                            color="red",
                            s=200,
                            marker="v",
                            label="Local Minima",
                            zorder=5,
                            edgecolor="darkred",
                            linewidth=2,
                        )

                        if len(locations) > 1:
                            ax.plot(
                                locations,
                                extrema_values,
                                color="red",
                                linestyle="--",
                                linewidth=2,
                                alpha=0.7,
                                label="Minima Trend Line",
                            )

                        for loc in locations:
                            ax.annotate(
                                f"{data_to_analyze[loc]:.2f}",
                                xy=(loc, data_to_analyze[loc]),
                                xytext=(0, -20),
                                textcoords="offset points",
                                bbox=dict(
                                    boxstyle="round,pad=0.3", fc="red", alpha=0.8
                                ),
                                arrowprops=dict(
                                    arrowstyle="->", connectionstyle="arc3,rad=0"
                                ),
                                fontsize=12,
                                color="white",
                                weight="bold",
                                ha="center",
                            )
                    else:
                        ax.scatter(
                            locations,
                            extrema_values,
                            color="green",
                            s=200,
                            marker="^",
                            label="Local Maxima",
                            zorder=5,
                            edgecolor="darkgreen",
                            linewidth=2,
                        )

                        if len(locations) > 1:
                            ax.plot(
                                locations,
                                extrema_values,
                                color="green",
                                linestyle="--",
                                linewidth=2,
                                alpha=0.7,
                                label="Maxima Trend Line",
                            )

                        for loc in locations:
                            ax.annotate(
                                f"{data_to_analyze[loc]:.2f}",
                                xy=(loc, data_to_analyze[loc]),
                                xytext=(0, 20),
                                textcoords="offset points",
                                bbox=dict(
                                    boxstyle="round,pad=0.3", fc="green", alpha=0.8
                                ),
                                arrowprops=dict(
                                    arrowstyle="->", connectionstyle="arc3,rad=0"
                                ),
                                fontsize=12,
                                color="white",
                                weight="bold",
                                ha="center",
                            )

                    ax.set_xlabel("Date", fontsize=14, fontweight="bold")
                    ax.set_ylabel("Price", fontsize=14, fontweight="bold")
                    ax.set_xticks(x_positions)
                    ax.set_xticklabels(x_labels, rotation=45, ha="right")
                    ax.grid(True, alpha=0.3, linestyle="--")
                    ax.legend(fontsize=12, loc="upper left")
                    ax.set_facecolor("#f8f9fa")
                    plt.tight_layout()

                    st.pyplot(fig)

                    extrema_data = [
                        {
                            "æ—¥æœŸ": labels_to_use[loc],
                            "åƒ¹æ ¼": data_to_analyze[loc],
                            "ç¯©é¸æ–¹å¼": extrema_display,
                        }
                        for loc in locations
                    ]

                    st.subheader("è©³ç´°è³‡è¨Š")
                    st.dataframe(pd.DataFrame(extrema_data), use_container_width=True)
    else:
        st.warning("Not enough price data for this stock to perform analysis.")
else:
    st.info("ğŸš« æ²’æœ‰è‚¡ç¥¨ç¬¦åˆæ‚¨çš„ç¯©é¸æ¢ä»¶")

    # Show current parameter settings for user reference
    st.markdown("**ç›®å‰è¨­å®š:**")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"- å¸‚å ´: **{stock_type}**")
        st.write(f"- ç¯©é¸æ–¹å¼: **{extrema_display}**")
        st.write(f"- æ¯”è¼ƒæ–¹å‘: **{reverse_display}**")
    with col2:
        st.write(f"- åº•/é ‚æ•¸é‡: **{num_extrema}**")
        st.write(f"- ç§»å‹•å¹³å‡: **{window_size} å¤©**")
        st.write(f"- é¡¯ç¤ºç¯„åœ: **{display_days} å¤©**")

# Add CSV Editor Section in Sidebar
st.sidebar.header("ğŸ“ è³‡æ–™ç®¡ç†")

with st.sidebar.expander("åˆªé™¤æ—¥æœŸç¯„åœè³‡æ–™"):
    st.info("åˆªé™¤æŒ‡å®šæ—¥æœŸç¯„åœçš„è‚¡åƒ¹è³‡æ–™æ¬„ä½")

    # Get current year for date context
    current_year = datetime.datetime.now().year

    # Date range input with date picker
    start_date_picker = st.date_input(
        "é–‹å§‹æ—¥æœŸ", value=datetime.date(current_year, 1, 1), help="é¸æ“‡é–‹å§‹æ—¥æœŸ"
    )

    end_date_picker = st.date_input(
        "çµæŸæ—¥æœŸ", value=datetime.date(current_year, 4, 1), help="é¸æ“‡çµæŸæ—¥æœŸ"
    )

    if start_date_picker and end_date_picker:
        # Get current dataframe
        current_df = (
            stock_fetcher.listed_data
            if stock_type == "ä¸Šå¸‚"
            else stock_fetcher.counter_data
        )

        # Get date columns
        date_columns = [col for col in current_df.columns if col not in BASE_COLUMNS]

        # Filter columns by date range (matching YYYY/MM/DD or MM/DD format)
        columns_to_delete = []
        for col in date_columns:
            try:
                # Try parsing as YYYY/MM/DD format first
                try:
                    col_date = datetime.datetime.strptime(col, "%Y/%m/%d").date()
                except ValueError:
                    # If that fails, try MM/DD format and assume year 2025
                    col_date = datetime.datetime.strptime(
                        f"2025/{col}", "%Y/%m/%d"
                    ).date()

                # Check if in range
                if start_date_picker <= col_date <= end_date_picker:
                    columns_to_delete.append(col)

            except (ValueError, IndexError):
                continue

        # Show preview and delete functionality
        if columns_to_delete:
            st.warning(f"âš ï¸ å°‡åˆªé™¤ {len(columns_to_delete)} å€‹æ—¥æœŸæ¬„ä½")

            # Show first few columns as preview
            preview_cols = columns_to_delete[:5]
            if len(columns_to_delete) > 5:
                st.write(
                    f"ğŸ“‹ é è¦½: {', '.join(preview_cols)}... (+{len(columns_to_delete)-5} æ›´å¤š)"
                )
            else:
                st.write(f"ğŸ“‹ å°‡åˆªé™¤: {', '.join(preview_cols)}")

            if st.button("ğŸ—‘ï¸ ç¢ºèªåˆªé™¤", type="secondary"):
                with st.spinner("æ­£åœ¨åˆªé™¤æ¬„ä½..."):
                    try:
                        # Delete and save
                        if stock_type == "ä¸Šå¸‚":
                            stock_fetcher.listed_data = stock_fetcher.listed_data.drop(
                                columns=columns_to_delete
                            )
                            stock_fetcher.listed_data.to_csv(
                                config.LISTED_CSV, index=False, encoding="utf-8-sig"
                            )
                        else:
                            stock_fetcher.counter_data = (
                                stock_fetcher.counter_data.drop(
                                    columns=columns_to_delete
                                )
                            )
                            stock_fetcher.counter_data.to_csv(
                                config.COUNTER_CSV, index=False, encoding="utf-8-sig"
                            )

                        st.success(f"âœ… æˆåŠŸåˆªé™¤ {len(columns_to_delete)} å€‹æ¬„ä½ï¼")
                        time.sleep(1)
                        st.rerun()

                    except Exception as e:
                        st.error(f"åˆªé™¤å¤±æ•—: {str(e)}")
        else:
            st.info("ğŸ“… æŒ‡å®šç¯„åœå…§ç„¡åŒ¹é…çš„æ—¥æœŸæ¬„ä½")

st.sidebar.markdown("---")
show_logout_button()
